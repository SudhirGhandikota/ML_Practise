{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import hashlib\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import OneHotEncoder, CategoricalEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline,FeatureUnion\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import svm\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function downloads the TGZ file from the given URL and extracts the contents(csv file)\n",
    "def download_file(url,dirname):\n",
    "    if not os.path.isdir(dirname):\n",
    "        os.makedirs(dirname)\n",
    "    outpath = os.path.join(dirname,\"housing.tgz\")\n",
    "    urllib.request.urlretrieve(url,outpath)\n",
    "    file_tgz = tarfile.open(outpath)\n",
    "    file_tgz.extractall(path=dirname)\n",
    "    file_tgz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function splits the data into random training and testing splits\n",
    "def split_train_test(data, test_ratio):\n",
    "    #generate random permutation of indices\n",
    "    random_indices = np.random.permutation(len(data))\n",
    "    test_size = int(len(data) * test_ratio)\n",
    "    test_indices = random_indices[:test_size]\n",
    "    train_indices = random_indices[test_size:]\n",
    "    return data.iloc[train_indices],data.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function checks if a given row(identified by the ID) should be part of training or test set based on last byte of the hash value\n",
    "def test_check(id_, test_ratio):\n",
    "    # if lasy byte of hash <256*test_ratio( eg: 51 if test_ratio = 20%)\n",
    "    return hashlib.md5(np.int64(id_)).digest()[-1] < 256*test_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function splits the data into training and testing splits by generating hashes for each data point\n",
    "#each data point is then assigned to training or testing test based on the last byte of the hash\n",
    "def split_train_test_id(data, test_ratio, id_col):\n",
    "    ids = data[id_col]\n",
    "    in_test_set = ids.apply(lambda id_: test_check(id_,test_ratio))\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"~/Desktop/Hands_on_ML/handson-ml/datasets/housing/housing.csv\"\n",
    "housing_df = pd.read_csv(file)\n",
    "housing_df.info() #gives a description of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df['ocean_proximity'].value_counts() #counts of each level for 'ocean_proximity' categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.describe() #summary of numerical attributes, ignoring NULL values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to show the plots inline\n",
    "%matplotlib inline\n",
    "#histograms of all numerical attributes with 50 bins each\n",
    "housing_df.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data into 80% - training, 20% - testing randomly\n",
    "train_set,test_set = split_train_test(housing_df,0.2)\n",
    "print(len(train_set),len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding an index column to the data frame\n",
    "housing_df = housing_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data into 80% - training, 20% - testing by assigning each sample to either training or testing set\n",
    "train_set,test_set = split_train_test_id(housing_df,0.2,\"index\")\n",
    "print(len(train_set),len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data into 80% - training, 20% - testing using pre-defined sklearn function\n",
    "#random_state sets a random generator seed. Same indices will be used for training and testing sets for any data of the same size\n",
    "train_set, test_set = train_test_split(housing_df, test_size=0.2, random_state = 42)\n",
    "print(len(train_set),len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Splitting the data into 80%-training, 20%-testing by stratifying over different income levels ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleting the index column created earlier\n",
    "housing_df.drop('index',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting median income(predictor variable) into categorical\n",
    "housing_df[\"income_cat\"] = np.ceil(housing_df[\"median_income\"] / 1.5) #dividing by 1.5 to limit number of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing any level >5 with 5\n",
    "#pandas.where replaces values where given condition is FALSE with given value\n",
    "housing_df[\"income_cat\"].where(housing_df['income_cat'] < 5,5.0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#StratifiedShuffleSplit = provides train/test indices to split data\n",
    "#n_splits = 1 indicates just one set of training and test indices\n",
    "split = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42)\n",
    "for train_index,test_index in split.split(housing_df,housing_df['income_cat']):\n",
    "    strat_train_set = housing_df.iloc[train_index]\n",
    "    strat_test_set = housing_df.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df['income_cat'].value_counts()/len(housing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_train_set['income_cat'].value_counts()/ len(strat_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test_set['income_cat'].value_counts()/ len(strat_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the categorical variable\n",
    "strat_train_set.drop(\"income_cat\",axis=1,inplace=True)\n",
    "strat_test_set.drop(\"income_cat\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_df = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots() #providing axes explicitly as a workaround for axis clipping issue\n",
    "#creating a scatter plot of longitude and latitude to visualize housing districts\n",
    "#alpha=0(transparent), alpha=1(opaque), s=marker_size as population size, c = color as median house value column\n",
    "housing_train_df.plot(kind=\"scatter\",x=\"longitude\",y=\"latitude\", alpha=0.4,\n",
    "                     s=housing_train_df['population']/100, label=\"population\",figsize=(10,8),\n",
    "                     c=\"median_house_value\",cmap=plt.get_cmap(\"jet\"),colorbar=True, ax=ax)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing pairwise-correlations (Pearson's) among all attributes\n",
    "corr_mat = housing_train_df.corr()\n",
    "corr_mat['median_house_value'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting pairwise scatter plots between selected numerical attributes\n",
    "attributes = [\"median_house_value\",\"median_income\",\"total_rooms\",\"housing_median_age\"]\n",
    "scatter_matrix(housing_train_df[attributes],figsize=(12,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatter plot between median_house_value and median_income\n",
    "housing_train_df.plot(kind=\"scatter\",x=\"median_income\", y=\"median_house_value\",alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating hybrid attributes based on existing ones\n",
    "housing_train_df['rooms_per_households'] = housing_train_df['total_rooms']/housing_train_df['households'] #no. of rooms per household\n",
    "housing_train_df['prop_bedrooms'] = housing_train_df['total_bedrooms']/housing_train_df['total_rooms'] #proportion of bedrooms all rooms\n",
    "housing_train_df['pop_per_household'] = housing_train_df['population']/housing_train_df['households'] #population per household"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing pairwise-correlations (Pearson's) among all attributes\n",
    "corr_mat = housing_train_df.corr()\n",
    "corr_mat['median_house_value'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating target labels and predictors\n",
    "housing_train_df = strat_train_set.drop(\"median_house_value\",axis=1) #drop creates a copy and does not affect the original df\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputing missing values of numeric attributes using the median\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "housing_train_num = housing_train_df.drop(\"ocean_proximity\",axis=1) #removing text attribute column\n",
    "imputer.fit(housing_train_num) #fit imputer instance to training data.Median values are calculated and stored in statistics_ instance variable\n",
    "X = imputer.transform(housing_train_num)\n",
    "housing_train_tr = pd.DataFrame(X, columns = housing_train_num.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting categorical variable into integed using factorize() method\n",
    "housing_train_cat = housing_train_df['ocean_proximity']\n",
    "housing_train_cat, housing_categories = housing_train_cat.factorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encoding of categorical variable\n",
    "encoder = OneHotEncoder(categories = \"auto\")\n",
    "#fit_transform() expects a 2D array. hence the 1D attribute array needs to be reshaped\n",
    "#reshape() allows one dimension to be -1, which means \"unspecified\": value is inferred from length of the array\n",
    "housing_train_cat_1hot = encoder.fit_transform(housing_train_cat.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_cat_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting sparse matrix to a Numpy dense array\n",
    "housing_train_cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_df = strat_train_set.drop(\"median_house_value\",axis=1) #drop creates a copy and does not affect the original df\n",
    "housing_train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## adding hybrid attributes user custom transformer classes ##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indexes of columns\n",
    "rooms_ix, bedrooms_ix, population_ix, household_ix = 4, 5, 6, 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BaseEstimator and TransformerMixin are base classes\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True): #no *args or **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        #creating the hybrid attributes\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, household_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n",
    "        else:\n",
    "            #concatenating the hybrid attributes to the existing ones\n",
    "            return np.c_[X, rooms_per_household, population_per_household]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiating CombinedAttributesAdder class\n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "#calling the transform method to add the new columns\n",
    "housing_extra_attribs = attr_adder.transform(housing_train_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ using pipelines to sequence the transformations ###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class to filter out(or pick) only the given attribute names\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attribs = list(housing_train_num)\n",
    "cat_attribs = ['ocean_proximity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating an sklearn pipeline with sequence of transformations for numerical attribues. \n",
    "#It takes a list of name/estimator pairs defining a sequence of steps\n",
    "#All but the last estimator must be transformers i.e they must have a fit_transform() method\n",
    "#pipelines fit() method when called, calls fit_transform() sequentially on all transformers, passing output of each call to the next call.\n",
    "#Until the final estimator is reached for which it just calls fit()\n",
    "num_pipeline = Pipeline([\n",
    "    ('selector',DataFrameSelector(num_attribs)),\n",
    "    ('imputer',SimpleImputer(strategy=\"median\")),\n",
    "    ('attribs_adder',CombinedAttributesAdder()),\n",
    "    ('std_scaler',StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating an sklearn pipeline with sequence of transformations for categorical attribues. \n",
    "#It takes a list of name/estimator pairs defining a sequence of steps\n",
    "cat_pipeline = Pipeline([\n",
    "    ('selector',DataFrameSelector(cat_attribs)),\n",
    "    ('cat_encoder',OneHotEncoder(categories = \"auto\",sparse=False))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining the two pipelines(for numerical and categorical attributes) using FeatureUnion class\n",
    "#It runs each transformer's (or entire transformer pipelines) transform() method in parallel, waits for their output and concatenates them\n",
    "full_pipeline = FeatureUnion(transformer_list = [\n",
    "    (\"num_pipeline\",num_pipeline),\n",
    "    (\"cat_pipeline\",cat_pipeline),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running the whole pipelines\n",
    "#housing_train_df.drop('index',axis=1,inplace=True)\n",
    "housing_final = full_pipeline.fit_transform(housing_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### Training a Linear Regression model #############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting a linear regression model\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_final, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing root mean squared error for the linear regression predictions on training data\n",
    "housing_predictions = lin_reg.predict(housing_final)\n",
    "lin_mse = mean_squared_error(housing_labels,housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "print(\"Room mean squared error in Dollars\",lin_rmse) #case of model underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing k-fold(k=10) Cross-Validation\n",
    "#cross_val_score expects a utility function(greater is better) than a cost function(lower is better) hence negative sign is used\n",
    "cv_scores = cross_val_score(lin_reg, housing_final, housing_labels, scoring=\"neg_mean_squared_error\",cv=10)\n",
    "lin_rmse_scores = np.sqrt(-cv_scores)\n",
    "print(lin_rmse_scores.mean(),lin_rmse_scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### Training a Decision tree regressor model #############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting a decision tree regressor\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(housing_final,housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing root mean squared error for the Decision tree regressor predictions\n",
    "housing_predictions = tree_reg.predict(housing_final)\n",
    "tree_mse = mean_squared_error(housing_labels,housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "print(tree_rmse) #case of model overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing k-fold(k=10) Cross-Validation\n",
    "#cross_val_score expects a utility function(greater is better) than a cost function(lower is better) hence negative sign is used\n",
    "cv_scores = cross_val_score(tree_reg, housing_final, housing_labels, scoring=\"neg_mean_squared_error\",cv=10)\n",
    "tree_rmse_scores = np.sqrt(-cv_scores)\n",
    "print(tree_rmse_scores.mean(),tree_rmse_scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### Training a Random forest regressor model #############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting a random forest regressor\n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(housing_final,housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing root mean squared error for the Decision tree regressor predictions\n",
    "housing_predictions = forest_reg.predict(housing_final)\n",
    "forest_mse = mean_squared_error(housing_labels,housing_predictions)\n",
    "forest_rmse = np.sqrt(forest_mse)\n",
    "print(forest_rmse) #case of model overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing k-fold(k=10) Cross-Validation\n",
    "#cross_val_score expects a utility function(greater is better) than a cost function(lower is better) hence negative sign is used\n",
    "cv_scores = cross_val_score(forest_reg, housing_final, housing_labels, scoring=\"neg_mean_squared_error\",cv=10)\n",
    "forest_rmse_scores = np.sqrt(-cv_scores)\n",
    "print(forest_rmse_scores.mean(),forest_rmse_scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### Training a SV Regression model #############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting a SV regressor - RBF kernel\n",
    "sv_reg = svm.SVR(gamma='scale')\n",
    "sv_reg.fit(housing_final,housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing root mean squared error for the Decision tree regressor predictions\n",
    "housing_predictions = sv_reg.predict(housing_final)\n",
    "sv_mse = mean_squared_error(housing_labels,housing_predictions)\n",
    "sv_rmse = np.sqrt(sv_mse)\n",
    "print(sv_rmse) #case of model overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing k-fold(k=10) Cross-Validation\n",
    "#cross_val_score expects a utility function(greater is better) than a cost function(lower is better) hence negative sign is used\n",
    "cv_scores = cross_val_score(sv_reg, housing_final, housing_labels, scoring=\"neg_mean_squared_error\",cv=10)\n",
    "sv_rmse_scores = np.sqrt(-cv_scores)\n",
    "print(sv_rmse_scores.mean(),sv_rmse_scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting a SV regressor - Linear kernel\n",
    "sv_reg = svm.SVR(gamma='scale',kernel='linear')\n",
    "sv_reg.fit(housing_final,housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing root mean squared error for the Decision tree regressor predictions\n",
    "housing_predictions = sv_reg.predict(housing_final)\n",
    "sv_mse = mean_squared_error(housing_labels,housing_predictions)\n",
    "sv_rmse = np.sqrt(sv_mse)\n",
    "print(sv_rmse) #case of model overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing k-fold(k=10) Cross-Validation\n",
    "#cross_val_score expects a utility function(greater is better) than a cost function(lower is better) hence negative sign is used\n",
    "cv_scores = cross_val_score(sv_reg, housing_final, housing_labels, scoring=\"neg_mean_squared_error\",cv=10)\n",
    "sv_rmse_scores = np.sqrt(-cv_scores)\n",
    "print(sv_rmse_scores.mean(),sv_rmse_scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving models(example)\n",
    "joblib.dump(sv_reg,'mymodel.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Performing a grid search over hyperparameters of Random forest regressor ###########################\n",
    "# Grid search  performs an exhaustive search on all possible combinations of paramaters (based on cartesian product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function retrieves the evaulations scores of the specific search type\n",
    "def show_search_scores(results):\n",
    "    #viewing all evaluation scores, not just the best ones\n",
    "    cvres = results.cv_results_\n",
    "    for mean_score, std, params in zip(cvres[\"mean_test_score\"], cvres[\"std_test_score\"], cvres[\"params\"]):\n",
    "        print(np.sqrt(-mean_score),np.sqrt(std),params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function retrieves the feature importance scores from the Random Forest regressor based on the best estimator from the search\n",
    "def show_feature_scores(results):\n",
    "    #retrieving the feature importances from the best estimator\n",
    "    feature_importances = results.best_estimator_.feature_importances_\n",
    "    extra_attribs = [\"rooms_per_hhold\",\"pop_pep_hhold\",\"bedrooms_per_room\"]\n",
    "    cat_encoder = cat_pipeline.named_steps[\"cat_encoder\"]\n",
    "    cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "    attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "    print(sorted(zip(feature_importances, attributes), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up two different parameter grids\n",
    "param_grid = [\n",
    "    {'n_estimators':[3, 10, 30], 'max_features': [2 ,4 ,6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiating a RandomForestRegressor\n",
    "forest_reg = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiating the GridSearch on the initiated RandomForestRegressor\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv = 5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(housing_final, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieving the best combination of parameters\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieving the best estimator\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewing all evaluation scores, not just the best ones\n",
    "show_search_scores(grid_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieving the feature importances from the best estimator\n",
    "show_feature_scores(grid_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Performing a randomized search over hyperparameters of Random forest regressor ###########################\n",
    "# Randomized search  picks a random combination of parameters. Number of random searchers depends on n_iter parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up two different parameter grids.\n",
    "#RandomizedSearch does not accept a list of parameter grids\n",
    "param_grid = {'n_estimators':[3, 10, 30], 'max_features': [2 ,4 ,6, 8]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiating the randomozed on the initiated RandomForestRegressor\n",
    "randomized_search = RandomizedSearchCV(forest_reg, param_grid, n_iter = 10, cv = 5, scoring='neg_mean_squared_error')\n",
    "randomized_search.fit(housing_final, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieving the best combination of parameters\n",
    "randomized_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewing all evaluation scores, not just the best ones\n",
    "show_search_scores(randomized_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieving the feature importances from the best estimator\n",
    "show_feature_scores(randomized_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Evaluating the model on the test set using the best model and hyperparameters from Grid search #####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting the best model based on grid search\n",
    "final_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing the test set by separating labels from attributes\n",
    "X_test = strat_test_set.drop(\"median_house_value\",axis = 1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running the transformation pipelines on the test set\n",
    "X_test_final = full_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the predictions on test set\n",
    "y_pred = final_model.predict(X_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting rmse on the test set\n",
    "final_mse = mean_squared_error(y_test, y_pred)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "print(final_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
